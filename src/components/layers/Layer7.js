
import React from 'react';

// eslint-disable-next-line
import Collapsible from 'react-collapsible';

// eslint-disable-next-line
import {ChangeMapLink, ChangeTopicLink} from './utils';

function Layer7(props) {

    return(
    // eslint-disable-next-line
    <div className="text-holder"><a class="anchorhead" name="top" href="#top"></a> <h1 id="methods">Methods</h1> <div class='toc'><ul><li><a href='#data-collection'>Data Collection</a></li><li><a href='#map-spatial-analysis'>Map: Spatial Analysis</a></li><li><a href='#themes-text-analysis'>Themes: Text Analysis</a></li><li><a href='#digital-platform'>Digital Platform</a></li></ul></div> <p>This layer provides a description of the methods used to collect the data for the project, the models used to perform text analysis of the life histories, and the underlying technologies used to create the digital project. All of the code and data are available for download under permissive open-source licenses. Links are provided within the text.</p> <a class="anchorhead" name="data-collection" href="#top"></a><h2><span>Data Collection</span><span class='lback'><a href='#top'>[top]</a></span></h2> <p>The collection of life histories used in this project are held by The Wilson Library at the University of North Carolina at Chapel Hill in the Southern Historical Collection.<a class="anchor" id="fnref1"></a><a href="#fn1" class="footnote-ref" role="doc-noteref"><sup>1</sup></a> The texts of the life histories have been digitized and made available for public access through the library’s web interface. Life histories are provided as PDF images; there are no searchable, machine-readable versions of the texts available on the website.<a class="anchor" id="fnref2"></a><a href="#fn2" class="footnote-ref" role="doc-noteref"><sup>2</sup></a> Most life histories have a short caption that includes information about the title of the interview, its location, date, and interviewee’s name. This information is not structured into specific fields. Additional metadata information about the life histories are included in the digitized images in the form of headers at the top of most interviews and as summary cards included in the archive. We structured the headers into a database for this project and created plain text machine-readable versions of each life history for data analysis.</p> <p>Creating metadata records of the interviews required manually parsing the unstructured text and reading the individual metadata headers for each interview. The process of manual parsing was conducted by the authors, students in a class taught by Rivard, and a paid research assistant. An article focused on the pedagogical practices and lessons learned from this process appeared in <em>Digital Humanities Quarterly</em>.<a class="anchor" id="fnref3"></a><a href="#fn3" class="footnote-ref" role="doc-noteref"><sup>3</sup></a> The article discusses various ways of crediting contributors—particularly how to credit those students who made substantive contributions beyond that of the typical class requirements—through authorship credit and other visualizations.</p> <p>After being converted into structure records, the metadata was organized into a collection of normalized tables.<a class="anchor" id="fnref4"></a><a href="#fn4" class="footnote-ref" role="doc-noteref"><sup>4</sup></a> These tables contain information about writers, revisers, interviewees, interviews, and professions. The collection of tables follows the "tidy data" model, with a different table dedicated to each type of record.<a class="anchor" id="fnref5"></a><a href="#fn5" class="footnote-ref" role="doc-noteref"><sup>5</sup></a> Normalized relational tables are particularly important in our dataset because many of the relationships linking tables to one another are complex one-to-many and many-to-many relationships. For example, most writers wrote more than one interview, and some interviews were co-written by two or more writers. The normalized data model guarantees that data about each entry is consistent and easy to update.</p> <p>While most of the variables in the metadata tables are relatively straightforward to record and describe, a few fields require some discussion. Some interviews used pseudonyms for the interviewee names. In many cases, the pseudonyms and real names are mapped to one another in the header of the typed interview; the digital archive hosted by The Wilson Library lists interviews by real names, not pseudonyms. For these two reasons, we have listed both forms of names when given in our dataset and use real names when displaying records on the digital site. This makes our data consistent with the archival source and does not reveal private data that is not available elsewhere.</p> <p>Recording race and gender information about writers and interviewees also require careful decision-making. As digital humanities scholars such as Catherine D’Ignazio and Lauren Klein have argued, "what gets counted counts," yet it must be done with careful attention to binaries, hierarchies, and classifications.<a class="anchor" id="fnref6"></a><a href="#fn6" class="footnote-ref" role="doc-noteref"><sup>6</sup></a> Because we were interested in understanding the gender and racial logic that the SLHP produced, we used the categories in the archival records.</p> <p>The SLHP subscribed to a gender binary of "male" or "female," which they often typed in the header of the life history. The gender of the interviewees and writers was inconsistently documented, though. Since we were interested in gender representation among the interviewees and writers, we assigned a gender based on the binary logic of the SLHP. Given how writers and editors wrote and edited the stories, the gender can usually be inferred based on pronouns and other gendered language (i.e. "wife") used in relation to an interviewee. The gender of the writers was determined by archival records through a close reading of correspondences for an individual’s pronouns. Given the number of writers who are silent in the archive except for the life history they wrote, we also turned to census data that used the gender binary.</p> <p>Along with gender, we were interested in how race was configured at the time. We took as our guide cautions about encoding racial logic through data that digital humanities scholars such as Jessica Marie Johnson and digital humanities initiatives such as #transformdh and #dhpoco have elucidated.<a class="anchor" id="fnref7"></a><a href="#fn7" class="footnote-ref" role="doc-noteref"><sup>7</sup></a> As constructed and unstable categories, racial categorization was a significant site of contention in the 1930s as groups debated the names and boundaries of race and ethnicity. For interviewees, we used the categories described in the interview metadata. The SLHP, we learned through the process of creating these data, used three primary categories: Negro, White, and Other. Because of the implications of the term "Negro" in the past and today, we used "Black" as the category that appears on the site. The categories, and process of creating the data, revealed how the SLHP writers primarily sought life histories along a Black/White binary or, in the words of W.E.B. DuBois, along "the color line" of the segregated South.<a class="anchor" id="fnref8"></a><a href="#fn8" class="footnote-ref" role="doc-noteref"><sup>8</sup></a> The category of "Other" indicated the disinterest in further classifying and identifying groups in the region. They were literally the "other" compared to the primary focus: people who resided in the region who identified along a Black/White binary.</p> <p>In an effort to account for the complicated relationship between race and ethnicity, the SLHP at times included categories such as Greek or Swedish. These categories should be used carefully, however, because the labels are inconsistently applied. For example, interviews of Greek families sometimes describe the interviewees as "White" and other times as "Greek." However, the focus on ethnic communities is, for the most part, specific projects that were commissioned by an outside group or repurposed from another FWP project, particularly from Social-Ethnic surveys. Drawing on concepts such as data feminism to use data to reveal marginalized voices, we included an additional field to capture more granular ethnic categories that can be inferred from the text.<a class="anchor" id="fnref9"></a><a href="#fn9" class="footnote-ref" role="doc-noteref"><sup>9</sup></a> We recognize that one must approach with caution the process of racial and ethnic categorization, but we hope that the data set at least offers metadata that disrupts the Black/White binary. We return to D’Ignazio and Klein’s point that "what gets counted counts" and queries to this dataset by ethnicity will render visible stories that were not prioritized.</p> <p>The racial logic of the SLHP extended to the hiring and assignments of writers. Writers who were identified as White were allowed to interview anyone, and writers who were identified as Black were primarily assigned to interview those who identified as Black. Racial information for the writers was determined by archival records. Records for which we were unable to determine a racial category are labeled as "unclear." As a result, the metadata about the interviews mostly reflect the logic of the SLHP and 1930s because the aim of this project was to understand the gender and racial formations of the SLHP that then shaped whose stories were (and were not) told and how.</p> <p>Our dataset includes a record for every interview that is listed in the archive’s finding aid. The digital files are organized into folders, which most frequently contain a single interview, but occasionally include over a dozen individual interviews. Some interviews are duplicates or near-duplicates of each other, and others consist of a single sentence indicating that the record has been deleted. For consistency and simplicity, our data split out each individual interview and includes duplicate records. Removal of duplicates and other processing is done during the analysis of the data, making explicit how modeling decisions are made relating to the data contained in the archive.</p> <p>Along with the metadata, we also produced machine-readable versions of the text of each interview. Off-the-shelf optical character recognition (OCR) made a reasonable first pass of some of the interviews but produced unusable text in others. An external paid service was used to manually clean up the OCR into usable text. Some typed records contain either typed or handwritten corrections made by the revisers and editors in the Southern Life History Project; these include fixing typos, rephrasing sentence structure, editorial comments about the content and quality of writing, and making substantive edits to the content of the interview. For consistency, and because the crossed-out text was often unreadable, the machine-readable files used only the corrected versions of the text without any handwritten editorial comments. In a limited number of cases, due to physical imperfections, fading, or issues with the digitization process, small portions of the texts were unreadable. These are marked with the phrase "[text not clear]." The final machine-readable text files include all of the header information contained in the typed pages but exclude page numbers and any written comments that are not corrections to the main text.</p> <p>The metadata and machine-readable versions of the life histories are published under the open-source GNU Public License (GPL-2).<a class="anchor" id="fnref10"></a><a href="#fn10" class="footnote-ref" role="doc-noteref"><sup>10</sup></a> These can be downloaded in bulk. All of the data are contained in plain text forms that can be read by most data analysis software. Metadata is provided as CSV files, and the texts are provided as text files (one file per interview). All of the material is encoded using UTF-8; the internal consistency of the records was checked with a set of unit tests.<a class="anchor" id="fnref11"></a><a href="#fn11" class="footnote-ref" role="doc-noteref"><sup>11</sup></a> The points on the interactive map and embedded figures in this project were all created using this metadata.</p> <a class="anchorhead" name="map-spatial-analysis" href="#top"></a><h2><span>Map: Spatial Analysis</span><span class='lback'><a href='#top'>[top]</a></span></h2> <p>The main visualization element when first landing on the project is a large interactive map of the southeastern United States. The default map shows the locations where interviews in the SLHP took place through the use of round dots overlaid on a minimal map of the United States. An analysis of the spatial elements of the collection shapes, in particular, the work in <span onClick={() => props.handlePage(4)}>Layer 3</span>, but is present throughout the entire project.</p> <p>Information about the location where each interview took place was included in the metadata discussed in the previous section. In the archival data, interviews have different levels of specificity of their spatial location. Almost all provide a state and city. Some include a neighborhood name or even an exact street address. For the map, we needed to associate each location with specific longitude and latitude coordinates. Because of the desired scale of the map, we worked only with city and state information. To accomplish the mapping to coordinates, we started by using a computational method through the GeoNames database.<a class="anchor" id="fnref12"></a><a href="#fn12" class="footnote-ref" role="doc-noteref"><sup>12</sup></a> When a city was found in the database, we used the listed longitude and latitude associated with the center of the city. After this process, we found two dozen cities that were not exact matches. For these, we manually figured out how to map the location to coordinates. Most of these were either small towns that no longer exist or unofficial locations (such as the name of a farm) that we were able to locate by looking closely at the interview text.</p> <p>On each of the interactive maps, there is a point for each location associated with an interview in a given category. The area of the point is proportional to the number of interviews in one location. The area of the point is fixed by the map distance, meaning that when the map is zoomed into, the circles become larger in absolute terms but cover (approximately) the same area of the map. We found through user testing that this approach was much easier to follow and visually engaging at high levels of zoom. Points on the map are given an opaque color so that points for nearby towns can be seen even when one town has a large number of interviews. On the maps which are colored in different colors based on metadata, such as the prolific interviewers, there are a few cases where multiple colors would need to be associated with the same location. To allow visitors to click on both points, we ensure that the larger dot is behind the smaller one. Further technical details about the construction of the interactive map in JavaScript are given in the Digital Platform section below.</p> <a class="anchorhead" name="themes-text-analysis" href="#top"></a><h2><span>Themes: Text Analysis</span><span class='lback'><a href='#top'>[top]</a></span></h2> <p><span onClick={() => props.handlePage(5)}>Layer 4</span>, alongside other archival evidence, presents several computational models to help understand and organize the 1,248 life histories in the collection. For these analyses, duplicated interviews and interviews whose text was removed from the archive were not included in the analysis. When two slightly different versions of an interview appeared in the collection, we selected the longest text. After filtering, the collection contained 1.106 life histories. Finally, these were cleaned to remove headers and instances of "[text not clear]" within the machine-readable text.<a class="anchor" id="fnref13"></a><a href="#fn13" class="footnote-ref" role="doc-noteref"><sup>13</sup></a></p> <p>The two text analysis models used in our analysis—topic modeling and document clustering—are both built to analyze term frequencies (TFs). TFs count how frequently particular words or word forms occur within each document within a corpus. To compute these counts, we passed the text of each life history through a language processing (NLP) pipeline using the R package cleanNLP.<a class="anchor" id="fnref14"></a><a href="#fn14" class="footnote-ref" role="doc-noteref"><sup>14</sup></a> The package applies prebuilt models to split the text into individual words and punctuation marks (tokenization), construct standardized forms of the words (lemmatization), and tag each word with a part-of-speech code.<a class="anchor" id="fnref15"></a><a href="#fn15" class="footnote-ref" role="doc-noteref"><sup>15</sup></a> Using this information, we constructed counts of lemmas for all lemmas tagged as nouns, verbs, adjectives, and adverbs.<a class="anchor" id="fnref16"></a><a href="#fn16" class="footnote-ref" role="doc-noteref"><sup>16</sup></a> Because of the heavy influence of place names in the lexicon of the texts, we also explicitly removed any place names (i.e. cities and census-designated places) contained in any of the geographic columns in the metadata.<a class="anchor" id="fnref17"></a><a href="#fn17" class="footnote-ref" role="doc-noteref"><sup>17</sup></a> The topic model and document cluster can be explored in the Theme Interface.</p> <p>One challenge with the life histories is the use of a significant amount of "eye dialect," in which words are intentionally misspelled to signal what are problematically called "nonstandard" pronunciations.<a class="anchor" id="fnref18"></a><a href="#fn18" class="footnote-ref" role="doc-noteref"><sup>18</sup></a> Examples include "git" for "get" and "wuz" for "was." The use of dialect is an interesting and important feature that we investigate in <span onClick={() => props.handlePage(5)}>Layer 4</span>. It offers insight into how spelling was used as a racial signifier and inculcated in White supremacist ideologies. At the same time, it dominates the signal within the topic models and document clusters, making it hard to detect other linguistic features. We have made available versions of our models with dialect included and with dialect removed for these reasons. To identify and remove dialect, we started by comparing each of the words identified by the NLP pipeline with a "standard" spelling dictionary of American English and removed words that were not included.<a class="anchor" id="fnref19"></a><a href="#fn19" class="footnote-ref" role="doc-noteref"><sup>19</sup></a> Then, we looked at the most overrepresented words in interviews of Black interviewees and manually constructed a list of additional dialect terms to remove. These consisted of relatively uncommon terms that are English words but have alternative meanings. For example, the word "den" was used heavily in the corpus as dialect for the word "then." Using this approach as a strategy to explore other linguistic features, the approach is intended to offer another way to explore the topics that do not reduce the stories of people of color to primarily racist applications of dialect.</p> <p>Using the term frequencies, we computed two sets of topic models, one with dialect terms and one without them. We used latent Dirichlet allocation (LDA) as implemented by the R package topic models to construct the models.<a class="anchor" id="fnref20"></a><a href="#fn20" class="footnote-ref" role="doc-noteref"><sup>20</sup></a> LDA is a common and well-known technique in digital humanities and digital history research that calculates a probability for word co-occurrence.<a class="anchor" id="fnref21"></a><a href="#fn21" class="footnote-ref" role="doc-noteref"><sup>21</sup></a> After some experimentation, we included 16 topics to display. The visualization of these topics on the digital project includes the probability distributions over words and the probability distributions of documents that each model defines.</p> <p>Finally, document clustering was also applied to each of the two sets of term frequencies. Document clustering is the process of grouping all of the items in a corpus of texts into discrete groups, called clusters, based on linguistic features. Clustering is not commonly used in digital humanities projects, primarily because of the momentum around the use of topic models. For many projects, including ours, it is useful to find groups of documents that use similar language features. It is possible to find some groups of documents by looking at the results of a topic model, but this approach will miss documents that cross between multiple topics. Also, the results of LDA are quite sensitive to a number of parameters, most notably the number of topics used. Document clustering can avoid these issues and move directly to the task of grouping together similar documents.</p> <p>We used spectral clustering to produce clusters of documents. Spectral clustering is a relatively well-known technique in statistical computing for grouping together textual documents.<a class="anchor" id="fnref22"></a><a href="#fn22" class="footnote-ref" role="doc-noteref"><sup>22</sup></a> We used the implementation from the R package casl to apply this algorithm.<a class="anchor" id="fnref23"></a><a href="#fn23" class="footnote-ref" role="doc-noteref"><sup>23</sup></a> Document clustering is a hierarchical clustering method. The algorithm starts by splitting all of the documents in a corpus into two groups such that the two groups differ as much as possible in their usage of words. Then, the same algorithm is applied to each of these groups separately to split the entire collection into 4 subgroups. Applying again yields 8 subgroups, then 16, and so forth. We applied this algorithm five times to yield a set of 32 clusters. Due to the iterative method, the clusters are related to one another. Documents in cluster 1 and cluster 2, for example, were split only in the final round of the algorithm.</p> <p>All of the code to produce the text analysis models is made available under the open-source GNU Public License (GPL-2).<a class="anchor" id="fnref24"></a><a href="#fn24" class="footnote-ref" role="doc-noteref"><sup>24</sup></a> The code works directly off of the data described in the previous section. It also includes the code to create the JSON files that serve as the backend for the digital project.</p> <a class="anchorhead" name="digital-platform" href="#top"></a><h2><span>Digital Platform</span><span class='lback'><a href='#top'>[top]</a></span></h2> <p>The project is a part of a growing community of scholars, publishers, and foundations working together to expand forms of academic scholarship, including what counts as evidence, ways of knowing, and communicating knowledge. Along with archival evidence, the creation, analysis, and communication of data sit at the core of this project. The digital platform offered an opportunity to make visible <em>what</em> kinds of data we created, <em>how</em> we analyzed the data, and <em>why</em> through visualizations and text. It also provided a space to communicate scholarship through visual ways of knowing—graphs, maps, and interactive visualizations. Additionally, the interactive visualizations encourage visitors to explore the archive alongside us, build off our scholarship, and pose their own questions. As Cox and Tilton have written, developing open access and interactive digital public projects can expand our argumentative strategies and reorient the reader/viewer as not just a person to be persuaded but as a participant engaged in humanistic inquiry and communication.<a class="anchor" id="fnref25"></a><a href="#fn25" class="footnote-ref" role="doc-noteref"><sup>25</sup></a></p> <p>The platform is designed to pair text and interactive visualizations to convey the project’s arguments and scope. The project is structured in layers. As a chapter, each layer offers insights into the social, political, and cultural work of the SLHP. In the same way that audiences have learned how to read a text to interpret an argument, audiences also have tools to interpret visualizations; there is a system of symbols and signifiers that people have learned to "read" visualizations that they employ daily. This project uses visualizations such as interactive mapping as a form of argumentation and then puts them in conversation with textual argumentation. As a result, this project is not strictly a textual book on a digital platform as we harness layers and the interpretive power of interactive visualizations to convey a set of arguments through an interactive platform made possible by the affordances of digital technologies.</p> <p>The structure of the project builds off the spatial and visual turn in digital humanities. As scholars such as Tara McPherson, Jentery Sayers, and Lev Manovich have argued, the digital humanities remains a text-heavy field.<a class="anchor" id="fnref26"></a><a href="#fn26" class="footnote-ref" role="doc-noteref"><sup>26</sup></a> The call to use the affordances of computational methods as visual ways of knowing, such as graphs and interactive visualizations, is amplifying. Led by scholars such as Stanford’s Richard White and over two decades of critical cartography, scholars have used visualizations such as maps to convey scholarly knowledge and arguments. Our project brings together these turns through the digital platform.</p> <p>The digital platform is written using a number of open-source technologies. The main functional elements of the site are written using the popular JavaScript framework ReactJS. Additional JavaScript packages were used within ReactJS to add specific functionality, such as the use of a router to create meaningful URLs and React-Dropdown to create interactive menus. Modern web standards are used to provide responsive, cross-platform compatible code using documented CSS3 and HTML5. The source code is available on GitHub.<a class="anchor" id="fnref27"></a><a href="#fn27" class="footnote-ref" role="doc-noteref"><sup>27</sup></a> Any original code produced for the project is under a GPL-2 license; some derivative components are released under an alternative open-source license as required by their respective authors.</p> <p>The mapping component of the website uses the JavaScript library Leaflet within ReactJS. Each of the maps was manually georeferenced using QGIS and projected into the Albers conic projection.<a class="anchor" id="fnref28"></a><a href="#fn28" class="footnote-ref" role="doc-noteref"><sup>28</sup></a> This projection preserves areas and more accurately represents distances between points when compared to a Mercator projection.<a class="anchor" id="fnref29"></a><a href="#fn29" class="footnote-ref" role="doc-noteref"><sup>29</sup></a> Map tiles were created using the Geospatial Data Abstraction Library (GDAL).<a class="anchor" id="fnref30"></a><a href="#fn30" class="footnote-ref" role="doc-noteref"><sup>30</sup></a> The map tiles are served locally by the project; this is slightly less efficient than a purpose-built GIS server but is more stable and reliable for long-term preservation and access to the digital project.</p> <p>The visualization of the topic models is written as a custom JavaScript code. It was adapted from a similar visualization produced for the Signs@40 project, made available under an open-source license.<a class="anchor" id="fnref31"></a><a href="#fn31" class="footnote-ref" role="doc-noteref"><sup>31</sup></a> All of the data for the visualizations are stored as JSON files to simplify deployment and increase the stability of our application. This design choice removes the need for a backend database, making it relatively easy to run the website from alternative sources. Keeping the backend of the website minimal also facilitates long-term access to the project by minimizing the ways that the site could become obsolete. Code to create the topic model files are included in the repository containing the code for producing the topic models.</p> <section class="footnotes footnotes-end-of-document" role="doc-endnotes"> <hr /> <ol> <li role="doc-endnote"><p><a class="anchor" id="fn1"></a>Federal Writers’ Project papers #3709, Southern Historical Collection, The Wilson Library, University of North Carolina at Chapel Hill.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li> <li role="doc-endnote"><p><a class="anchor" id="fn2"></a>For more, see <a target="_blank" rel="noopener noreferrer" href="https://catalog.lib.unc.edu/catalog/UNCb2431314">[link]</a>.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li> <li role="doc-endnote"><p><a class="anchor" id="fn3"></a>Courtney Rivard, Taylor Arnold, Lauren Tilton, "Building Pedagogy into Project Development: Making Data Construction Visible in Digital Projects," <em>Digital Humanities Quarterly</em> 13, no. 2 (2019).<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li> <li role="doc-endnote"><p><a class="anchor" id="fn4"></a>We recognize that there are debates about the term "normalized." In this context, we use the word to indicate a specific approach to structuring, which is referred to as "database normalization."<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li> <li role="doc-endnote"><p><a class="anchor" id="fn5"></a>Hadley Wickham, "Tidy Data," <em>Journal of Statistical Software</em> 59, no. 10 (2014).<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li> <li role="doc-endnote"><p><a class="anchor" id="fn6"></a>Catherine D’Ignazio and Lauren Klein, "What Gets Counted Counts," Data Feminism, March 16, 2020, <a target="_blank" rel="noopener noreferrer" href="https://data-feminism.mitpress.mit.edu/pub/h1w0nbqp">[link]</a>. See also Elizabeth Losh and Jacqueline Wernimont, <em>Bodies of Information: Intersectional Feminism and the Digital Humanities</em> (University of Minnesota Press, 2019).<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li> <li role="doc-endnote"><p><a class="anchor" id="fn7"></a>Jessica Marie Johnson, "Markup Bodies: Black [Life] Studies and Slavery [Death] Studies at the Digital Crossroads," <em>Social Text</em> 36, no. 4 (2018): 57–79. DOI: <a target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1215/01642472-7145658">10.1215/01642472-7145658</a>. See also Amy E. Earhart and Toniesha L. Taylor, "Pedagogies of Race: Digital Humanities in the Age of Ferguson," in <em>Debates in the Digital Humanities 2016</em>, ed. Matthew K. Gold and Lauren F. Klein, 251–64 (University of Minnesota Press, 2016); Jennifer Mahoney et al. "Data Fail: Teaching Data Literacy with African Diaspora Digital Humanities," <em>Journal of Interactive Technology &amp; Pedagogy</em>, December 10, 2020. https://jitp.commons.gc.cuny.edu/data-fail-teaching-data-literacy-with-african-diaspora-digital-humanities/; Safiya Umoja Noble, "Toward a Critical Black Digital Humanities," in Debates in the Digital Humanities 2019, ed. Matthew K. Gold and Lauren F. Klein, 25–35 (University of Minnesota Press, 2019).<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li> <li role="doc-endnote"><p><a class="anchor" id="fn8"></a>W.E.B. Du Bois, <em>The Souls of Black Folk</em> (Open Road Integrated Media, Inc., 1994), 5<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li> <li role="doc-endnote"><p><a class="anchor" id="fn9"></a>These are not exhaustive, focusing most on flagging a few interviews of people of Greek and Cuban descent.<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li> <li role="doc-endnote"><p><a class="anchor" id="fn10"></a>Taylor Arnold, Emeline Blevins Alexander, Courtney Rivard, Lauren Tilton, , and Laura Wexler, "FWP Life History Project in the American South: Machine-Readable Text and Metadata (Version 1.0)," Zenodo, May 29, 2020, <a target="_blank" rel="noopener noreferrer" href="http://doi.org/10.5281/zenodo.3865765">10.5281/zenodo.3865765</a>.<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li> <li role="doc-endnote"><p><a class="anchor" id="fn11"></a>The tests ensure, for example, that all of the table keys link together and that the interviews appear to correctly match their metadata (i.e. contains the title of the interview in the text and has a reasonable number of words given the number of pages).<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li> <li role="doc-endnote"><p><a class="anchor" id="fn12"></a>Dirk Ahlers, "Assessment of the Accuracy of GeoNames Gazetteer Data," <em>Proceedings of the G.I.R. Workshop</em>, 2013, pp. 74–81, CiteSeerX 10.1.1.722.8740.<a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li> <li role="doc-endnote"><p><a class="anchor" id="fn13"></a>For more about debates on "cleaning" data, see Katie Rawson and Trevor Muñoz, "Against Cleaning," in <em>Debates in the Digital Humanities 2019,</em> eds. Lauren Klein and Matt Gold (University of Minnesota Press, 2019), <a target="_blank" rel="noopener noreferrer" href="https://dhdebates.gc.cuny.edu/read/untitled-f2acf72c-a469-49d8-be35-67f9ac1e3a60/section/07154de9-4903-428e-9c61-7a92a6f22e51">[link]</a>.<a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li> <li role="doc-endnote"><p><a class="anchor" id="fn14"></a>Taylor Arnold. "A Tidy Data Model for Natural Language Processing Using cleanNLP," <em>The R Journal</em> 9, no. 2 (2017): 248–26.<a href="#fnref14" class="footnote-back" role="doc-backlink">↩︎</a></p></li> <li role="doc-endnote"><p><a class="anchor" id="fn15"></a>Matthew Honnibal, Ines Montani, Sofie Van Landeghem, and Adriane Boyd, "spaCy: Industrial-strength Natural Language Processing in Python," Zenodo, 2020, 10.5281/zenodo.1212303.<a href="#fnref15" class="footnote-back" role="doc-backlink">↩︎</a></p></li> <li role="doc-endnote"><p><a class="anchor" id="fn16"></a>Ann Bies, Justin Mott, Colin Warner, and Seth Kulick, "English Web Treebank LDC2012T13," <em>Linguistic Data Consortium</em>, 2012, <a target="_blank" rel="noopener noreferrer" href="https://doi.org/10.35111/m5b6-4m82">10.35111/m5b6-4m82</a>.<a href="#fnref16" class="footnote-back" role="doc-backlink">↩︎</a></p></li> <li role="doc-endnote"><p><a class="anchor" id="fn17"></a>These should have been removed by the part of speech tags, which have a separate category for proper nouns, but errors are common with this process, particularly when proper capitalization is not used (not uncommon in indirect quotes in this corpus).<a href="#fnref17" class="footnote-back" role="doc-backlink">↩︎</a></p></li> <li role="doc-endnote"><p><a class="anchor" id="fn18"></a>Toni Morrison, <em>Playing in the Dark: Whiteness and the Literary Imagination</em> (Harvard University Press, 1992).<a href="#fnref18" class="footnote-back" role="doc-backlink">↩︎</a></p></li> <li role="doc-endnote"><p><a class="anchor" id="fn19"></a>L. Németh, V. Trón, P. Halácsy, A. Kornai, A. Rung, and I. Szakadát, "Leveraging the Open-Source ispell Codebase for Minority Language Analysis," <em>Proceedings of SALTMIL</em>, 2004, 56–59.<a href="#fnref19" class="footnote-back" role="doc-backlink">↩︎</a></p></li> <li role="doc-endnote"><p><a class="anchor" id="fn20"></a>K. Hornik and B. Grün, "Topic Models: An R Package for Fitting Topic Models," <em>Journal of Statistical Software</em> 40, no. 13: 1–30.<a href="#fnref20" class="footnote-back" role="doc-backlink">↩︎</a></p></li> <li role="doc-endnote"><p><a class="anchor" id="fn21"></a>For a good technical overview focused on humanities applications, see David Blei, "Topic Modeling and Digital Humanities," <em>Journal of Digital Humanities</em> 2, no. 1 (2012).<a href="#fnref21" class="footnote-back" role="doc-backlink">↩︎</a></p></li> <li role="doc-endnote"><p><a class="anchor" id="fn22"></a>William Donath and Alan Hoffman, "Algorithms for Partitioning of Graphs and Computer Logic Based on Eigenvectors of Connections Matrices," <em>I.B.M. Technical Disclosure Bulletin</em>, 1972.<a href="#fnref22" class="footnote-back" role="doc-backlink">↩︎</a></p></li> <li role="doc-endnote"><p><a class="anchor" id="fn23"></a>Taylor Arnold, Michael Kane, and Bryan Lewis, <em>A Computational Approach to Statistical Learning</em> (Chapman &amp; Hall/C.R.C. Texts in Statistical Science, 2019).<a href="#fnref23" class="footnote-back" role="doc-backlink">↩︎</a></p></li> <li role="doc-endnote"><p><a class="anchor" id="fn24"></a>"FWP Life History Project in the American South: Text Analysis Code," GitHub, <a target="_blank" rel="noopener noreferrer" href="https://github.com/statsmaths/fwp-life-histories-analysis">[link]</a>. Accessed February 28 2021.<a href="#fnref24" class="footnote-back" role="doc-backlink">↩︎</a></p></li> <li role="doc-endnote"><p><a class="anchor" id="fn25"></a>Jordana Cox and Lauren Tilton, "The Digital Public Humanities: Giving New Arguments and New Ways to Argue," <em>Review of Communication</em> 19, no. 2: 127–46, DOI: 10.1080/15358593.2019.1598569<a href="#fnref25" class="footnote-back" role="doc-backlink">↩︎</a></p></li> <li role="doc-endnote"><p><a class="anchor" id="fn26"></a>Lev Manovich, <em>Cultural</em> <em>Analytics</em> (M.I.T. Press, 2021); Tara McPherson, "Introduction: Media Studies and the Digital Humanities," <em>Cinema Journal</em> 48, no. 2 (2009): 119–23. Accessed March 1, 2021. <a target="_blank" rel="noopener noreferrer" href="http://www.jstor.org/stable/20484452">[link]</a>; Jentery Sayers, ed. <em>The Routledge Companion to Media Studies and Digital Humanities</em> (Routledge, 2018).<a href="#fnref26" class="footnote-back" role="doc-backlink">↩︎</a></p></li> <li role="doc-endnote"><p><a class="anchor" id="fn27"></a><a target="_blank" rel="noopener noreferrer" href="https://github.com/statsmaths/writing-their-voices">[link]</a><a href="#fnref27" class="footnote-back" role="doc-backlink">↩︎</a></p></li> <li role="doc-endnote"><p><a class="anchor" id="fn28"></a>James Gray, "Getting Started with Quantum G.I.S." <em>Linux Journal</em>, March 26, 2008, http://www.linuxjournal.com/content/getting-started-quantum-gis.<a href="#fnref28" class="footnote-back" role="doc-backlink">↩︎</a></p></li> <li role="doc-endnote"><p><a class="anchor" id="fn29"></a>J. A. Kimerling, S. W. Overton, and D. White, "Statistical Comparison of Map Projection Distortions within Irregular Areas," <em>Cartography and Geographic Information Systems</em> 22, no. 3 (1995): 205–21.<a href="#fnref29" class="footnote-back" role="doc-backlink">↩︎</a></p></li> <li role="doc-endnote"><p><a class="anchor" id="fn30"></a>F. Warmerdam, "The Geospatial Data Abstraction Library," in <em>Open-Source Approaches in Spatial Data Handling</em>, ed. G. Brent Hall and Michael G. Leahy, pp. 87-104. Springer, 2008.<a href="#fnref30" class="footnote-back" role="doc-backlink">↩︎</a></p></li> <li role="doc-endnote"><p><a class="anchor" id="fn31"></a>Andrew Goldstone, Susana Galán, C. Laura Lovin, Andrew Mazzaschi, and Lindsey Whitmore. "An Interactive Topic Model of Signs," GitHub, <a target="_blank" rel="noopener noreferrer" href="https://github.com/signs40th/topic-model">[link]</a>.<a href="#fnref31" class="footnote-back" role="doc-backlink">↩︎</a></p></li> </ol> </section>  </div>
    )
}

export {Layer7};
